# NIC-2015-Pytorch
This project is the Pytorch implementation of Neural Image Captioning 2015 paper by Vinyals et. al.<a href = "https://arxiv.org/abs/1411.4555">[PDF]</a>. The implementation is inspired from the Udacity Image captioning project <a href = "https://github.com/udacity/CVND---Image-Captioning-Project">[Repo link]</a></br>
<ul>
  <li><b>Backend :</b> Pytorch, Pytorch Vision</li>
  <li><b>Dataset :</b> MS COCO 2014 Dataset <a href = "http://cocodataset.org/#download">[Link]</a></li>
</ul>
<b>Model Architecture:</b>
<center><img src = "images/model_architecture.png" height = "400px" width = "550px"></center>
<b>File Description:</b>
<ul>
  <li><b>data_load.py :</b> Dataloader class and functions for data augmentation.</li>
  <li><b>model.py :</b> Model class consisting of model definitions and functions.</li>
  <li><b>vocabulary.py :</b> Model class consisting of vocublary functions.</li>
  <li><b>training.ipynb :</b> Jupyter notebook with training hyperparameters like learning rate, batch size, embedding size, hidden state size etc.</li>
  <li><b>inference.ipynb :</b> Jupyter notebook to sample the captions generated by the encoder-decoder model.</li>
  <li><b>vocabulary and architecture experiments.ipynb :</b> Jupyter notebook to understand the vocabulary generation process and experiment with the CNN-RNN architecture to check whether the model.py implementation is correct or not.</li>

</ul>
