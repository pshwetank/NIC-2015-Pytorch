# NIC-2015-Pytorch
This project is the Pytorch implementation of Neural Image Captioning 2015 paper by Vinyals et. al.<a href = "https://arxiv.org/abs/1411.4555">[PDF]</a>. The implementation is inspired from the Udacity Image captioning project <a href = "https://github.com/udacity/CVND---Image-Captioning-Project">[Repo link]</a></br>
<ul>
  <li><b>Backend :</b> Pytorch, Pytorch Vision</li>
  <li><b>Dataset :</b> MS COCO 2014 Dataset <a href = "http://cocodataset.org/#download">[Link]</a></li>
</ul>
# Architecture
<center><img src = "images/model_architecture.png" height = "400px" width = "550px"></center>
# File Description
<ul>
  <li><b>data_load.py :</b> Dataloader class and functions for data augmentation.</li>
  <li><b>model.py :</b> Model class consisting of model definitions and functions.</li>
  <li><b>vocabulary.py :</b> Model class consisting of vocublary functions.</li>
  <li><b>training.ipynb :</b> Jupyter notebook with training hyperparameters like learning rate, batch size, embedding size, hidden state size etc.</li>
  <li><b>inference.ipynb :</b> Jupyter notebook to sample the captions generated by the encoder-decoder model.</li>
  <li><b>vocabulary and architecture experiments.ipynb :</b> Jupyter notebook to understand the vocabulary generation process and experiment with the CNN-RNN architecture to check whether the model.py implementation is correct or not.</li>

</ul>
# Dataset setup instructions
Please follow these instructions to setup the MS COCO 2014 dataset for training. Remember, the training dataset is 13GB along with test data(6GB). Before downloading, ensure good bandwidth and enough storage(atleast 20 GB for dataset) on server.
 
1. Clone this repo: https://github.com/cocodataset/cocoapi  
```
git clone https://github.com/cocodataset/cocoapi.git  
```

2. Setup the coco API (also described in the readme [here](https://github.com/cocodataset/cocoapi)) 
```
cd cocoapi/PythonAPI  
make  
cd ..
```
3. Download some specific data from here: http://cocodataset.org/#download (described below)

* Under **Annotations**, download:
  * **2014 Train/Val annotations [241MB]** (extract captions_train2014.json and captions_val2014.json, and place at locations cocoapi/annotations/captions_train2014.json and cocoapi/annotations/captions_val2014.json, respectively)  
  * **2014 Testing Image info [1MB]** (extract image_info_test2014.json and place at location cocoapi/annotations/image_info_test2014.json)

* Under **Images**, download:
  * **2014 Train images [83K/13GB]** (extract the train2014 folder and place at location cocoapi/images/train2014/)
  * **2014 Val images [41K/6GB]** (extract the val2014 folder and place at location cocoapi/images/val2014/)
  * **2014 Test images [41K/6GB]** (extract the test2014 folder and place at location cocoapi/images/test2014/)
